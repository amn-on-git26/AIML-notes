----

# Here is the google collab NOTEBOOK LINK:

[https://colab.research.google.com/drive/1OuJA1KC2IUexv0TXGkkQTTl1B-kJKV-P?usp=sharing](https://colab.research.google.com/drive/1OuJA1KC2IUexv0TXGkkQTTl1B-kJKV-P?usp=sharing)

------

**Important Things to decode from the collab**
-----
Notes: 
Things that we need to discuss

# Part 1:The XOR Problem

### Why XOR?
---

XOR (exclusive or) is a simple logical operarion:


* If inputs are different-> output 1

* If inputs are the same -> output 0

  <img width="349" height="245" alt="Screenshot 2026-02-23 110521" src="https://github.com/user-attachments/assets/4bff67e0-bc2f-4a98-8106-92fd3c4da6b7" />
<img width="349" height="245" alt="Screenshot 2026-02-23 110521" src="https://github.com/user-attachments/assets/4bff67e0-bc2f-4a98-8106-92fd3c4da6b7" />
----

## The Historical Importance
----

In 1969, Minsky and Papert proved that a single-layer perceptron (one neuron) cannot learn XOR. This caused the first "AI Winter" â€” people thought neural networks were fundamentally limited.

The solution? Hidden layers. A network with at least one hidden layer CAN learn XOR. This notebook proves it.


